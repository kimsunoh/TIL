HW #2
1. lecture 3를 듣고 다음 *키워드*에 대해 설명할 수 있게 준비해주세요.
http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf
keyword: linear classification, support vector machine, loss, regularization, sigmoid, softmax, optimization, gradient descent, stochastic gradient descent
+ 추가 학습 : rvm, histogram of oriented gradient, bag of words
+ svm 을 공부하실 때 참고하면 좋은 블로그 : http://jaejunyoo.blogspot.com/2018/01/support-vector-machine-1.html
이 블로그에서 마지막에 설명한 kernel method에 관해 더 공부하고 싶으시다면 prml 의 6, 7 단원을 참고하세요.

2. tensorflow 를 설치하고 1과 2를 더하는 graph를 constant, variable, placeholder 를 이용한 세 가지의 방법으로 만든 뒤 각각 출력해보세요.

---
# linear classification (선형분류)
- 개체의 속성을 linear combination 수치를 바탕으로 개체가 속하는 그룹, 또는 클래스를 판별하는 분류법
- 클래스를 -1 or 1 으로 판단하는 것

# support vector machine (SVM)
- 범주를 나누는 분류 문제를 푼다고 할때, 두 범주를 구분하는 경계면이 있을 때, 경계면과 데이터 사이의 거리를 Margin이라 하는데, 이때 이 마진을 최대화 하는 분류 경계면을 찾는 기법

# loss
- 가중치 w에 대한 어떤  것이 좋고 나쁜지에 대한 불량을 수치를 구하는 함수
- 내 모델을 통해 생성된 결과 값과 실제로 발생하기를 원했던 값간의 차이를 계산하는 함수. 목적에 따라 여러 종류의 함수가 존재할 수 있다.
- Hinge loss
	- 1을 넘어가면서, 0 으로 수렴하는 것을 볼 수 있다. 

# regularization

# activation function (활성함수)
- 뉴럴네트워크의 개별 뉴런에 들어오는 입력신호의 총합을 출력신호로 변환하는 함수

# sigmoid (== 로지스틱 함수  )
- L = 1이고, midpoint가 0
- 특징
	- 계단함수와는 다른 연속성을 갖게됨
	- 성공과 실패를 구분하는 부분은 경사가 급하고 나머지 부분에서는 경사가 완만하다.
		- 두 평행선이 점근선이고 치역은 (0,1)이다. 즉 위와 같은 활성함수의 함숫값은 성공확률이라는 의미로 해석할 수 있다(1을 실패라고 정의했다면 실패 확률로 해석하면 된다).
	- logistic함수의 특징은 x가 어떤값이어도 바로 1혹은 0으로 값을 얻어낼수 있다는 것이다. 주로 어떤 현상을 단순화하여 1과 0으로 놓고, 그 사이값으로 확률을 추론하는데에 많이 사용된다
- 선형함수를 사용하면 층을 깊게 만드는 의미가 없어진다.


# softmax
- 뉴런의 출력값에 대하여 class 분류를 위하여 마지막 단계에서 출력값에 대한 정규화를 해주는 함수활성화 함수
- 특징
	- 모든 입력신호로부터 영향을 받는다는 특징이 있음
	- 출력 값이 0~1 사이의 실수이며, 결과값의  총합은 1이다
	- 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다.

# optimization (최적화)
- 목적은 이 손실함수(loss function)을 최소화시키는 W을 찾아내는 것이다

# gradient descent (경사하강법)
- 손실함수(loss function)의 그라디언트(gradient)와 관계있다. 눈 가리고 하산하는 것에 비유할 때, 발 밑 지형을 잘 더듬어보고 가장 가파르다는 느낌을 주는 방향으로 내려가는 것에 비견할 수 있다.
- 느리고 근사값이지만 쉬운 방법 

# stochastic gradient descent
- 빠르고 정확하지만 미분이 필요하고 실수하기 쉬운 방법
-  유한차이(finite difference)를 이용해서 매우 단순하지만, 단점은 근사값이라는 점과 (그라디언트의 진짜 정의는 “h”가 0으로 수렴할 때의 극한값인데, 여기서는 그냥 작은 “h”값을 쓰기 때문에), 계산이 비효율적이라는 것이다.

---
## 참고링크
- [Stanford 2017 cs231n lecture 3](https://www.youtube.com/watch?v=h7iBpEHGVNc&index=3&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)
- [딥러닝 학습 기술들](https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/)
- [optimization and gradient stanford](http://aikorea.org/cs231n/optimization-1/)