# Basic Perceptron (단순 퍼셉트론)
: 프랑크 로젠블라트가 1957년에 고안한 알고리즘
- 딥러닝(신경망)의 기본(소양) 알고리즘
- 다수의 신호를 입력으로 받아 하나의 신호를 출력함
	- 복수의 입력 신호에 각각 고유한 가중치를 부여함
	- y = x1w1 + w2x2 > theta ? 1 : 0
		- x1,x2가 입력일 때 각각 w1,w2라는 가중치를 부여함
		- 입력과 가중치 합과 theta(임계값)의 비교해 뉴런의 활성화 여부를 결정
- 퍼셉트론 신호는 1/0 두가지 값을 가짐
	- 1 : 흐른다, 0 : 안 흐른다
- 계단 함수를 활성화 함수로 사용한 모델
	- 계단 함수 : 임계값을 경계로 출력이 바뀌는 함수 

## 논리 회로의 가중치, 임계값 설정
### AND 
- 두개다 true일 경우 뉴런이 활성화됨
``` python
def AND(x1, x2):
	w1, w2, theta = 0.5, 0.5, 0.7
	result = x1*w1 + x2*w2
	if result > theta:
		return 1
	else:
		return 0

print( AND(1,1) )
# 1
print( AND(0,1) )
# 0
```

### NAND
: Not AND
- AND 게이트를 구현하는 매개변수의 부호를 모두 반전하면 구할 수 있음

### OR
- 입력신호 둘 중 하나만 true이어도 뉴런이 활성화됨

## weight(가중치)와 bias(편향) 도입
### bias (편향)
- 퍼센트론은 '입력 신호에 가중치를 곱한값과 편향의 합' > 0이면 1, 아니면 0을 출력한다
	- b + x1w1 + w2x2 > 0 ? 1 : 0
		- b == -theta
``` python
import numpy as np 

x = np.array([0, 1])
w = np.array([0.5, 0.5])
b = -0.7

print( w*x )
# [ 0.   0.5]

print( np.sum( w*x ) )
# 0.5

print( np.sum( w*x ) + b 
# -0.2, -0.19999999999999996 ( 부동소수점수에 대한 오류로 앞의 갑이 나옴 )
```

#### bias AND, NAND, OR 게이트
``` python
def AND(x1, x2):
	x = np.array([x1, x2])
	w = np.array([0.5, 0.5])
	b = -0.7

	result = b + np.sum( x * w )
	if result > 0:
		return 1
	else:
		return 0

def NAND(x1, x2): # AND와 weight, bias만 다름
	x = np.array([x1, x2])
	w = np.array([-0.5, -0.5])
	b = 0.7
	
	result = b + np.sum( x * w )
	if result > 0:
		return 1
	else:
		return 0

def OR(x1, x2): # AND와 weight, bias만 다름
	x = np.array([x1, x2])
	w = np.array([0.5, 0.5])
	b = -0.2
	
	result = b + np.sum( x * w )
	if result > 0:
		return 1
	else:
		return 0
```

## 다층 퍼셉트론 ( Multi-layer perceptron )
- 단층 퍼셉트론으로 표현할 수 없는 것을 층(연산의 중복)을 만들어 표현하는 것
- 층수는 가중치를 기준으로 계산
	- XOR 게이트는 2층 

### 퍼셉트론의 한계
- 직선 하나로 나눈 영영만 표현할 수 있음

#### XOR 게이트
- 배타적 논리합이라는 논리 회로
	- x1, x2중 한쪽만 1일때 true
- 단층 퍼셉트론으로는 표현 불가, 다층퍼셉트론으론 표현 가능

### 선형과 비선형
- XOR 게이트를 그래프로 나타낸다면, 곡선형 그래프로 나타남
	- 직선하나로 표현할 수 없음

### 다층 퍼셉트론 XOR
``` python
def XOR(x1, x2):
	s1 = NAND(x1,x2)
	s2 = OR(x1,x2)
	return AND(s1,s2)
```


# 신경망
- 여러층으로 구성되고 깔끔한 활성화 함수를 사용하는 네트워크
- 입력층, 출력층, 은닉층으로 나누어져 있음
	- 입력층
	- 출력층
		- 층수를 셀때는 출력층을 0층으로 셈
	- 은닉층
		- 사람 눈에는 보이지 않음

- 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력
- 활성함수를 이용

## Activation function (활성 함수)
- 식으로 표현
	- 신호의 총합을 출력 신호로 변환하는 함수
		- x : b + x1w1 + w2x2 > 0
		- h(x) > 0 ? 1 : 0
			- h(x)를 활성 함수라 함
	- 조건분기의 동작을 함수로 나타낼 경우 h(x)로 표현	
- 활성화 함수를 계단 함수가 아닌 다른 함수를 사용함
	- 퍼셉트론에서는 활성화 함수로 계단 함수를 이용함
	- 퍼셉트론의 계단 함수를 다른 활성화 함수로 변화하는 것이 신경망
	- 신경망과 퍼셉트론의 가장 큰 차이가 '활성화 함수'

### 용어
- 가중치가 계산된 x는 '노드'라 부름
	- 노드 == 뉴런

### 신경망의 활성화 함수
#### 시그모이드 함수
: sigmoid(시그모이드) == S자 모양이라는 뜻
- h(x) = 1 / 1 + exp(-x)
	- exp(-x) == e^-x, e는 자연상수
- 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환함
	- 변환된 신호는 다음 노드로 전달함
- 신경망에서는 시그모이드 함수를 사용하며, 계단함수와는 다른 연속성을 갖게됨
	- 입력값에 비례해 출력값을 조절함
- 구현
``` python
def sigmoid(x):
	return 1 / 1 + np.exp(-x)

y = np.array( [10, 3, -1, -5] )

print( sigmoid(y) )
#[   1.0000454     1.04978707    3.71828183  149.4131591 ]
```

