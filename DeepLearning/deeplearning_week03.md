# 신경망
## 활성화 함수
### ReLU 함수
- 입력이 0을 넘으면 그 입력을 그대로 출력, 0이하면 0을 출력하는 함수
- h(x) = x > 0 ? x : 0

## 다차원 배열의 계산

### 행렬 곱 ( 행렬의 내적 )
- np.dot( list1, list2 )
	- np.dot( l1, l2 ) != np.dot( l2, l1 )
	- list1의 열 수와 list2의 행 수가 같아야함

### 신경망의 내적
```python
x = np.array( [1,2] )
w = np.array( [[1,3,5],[2,4,6]] )

x.shape
# (2,)
w.shape
# (2,3)

print( np.dot(x,w) )
# [5 11 17]
```

## 츨력층 설계하기
- 신경망은 분류와 회귀 모두에 이용할 수 있음
	- 회귀에는 항등함수, 분류는 소프트맥스 함수를 사용함
		- 분류 : 데이터가 어떤 분류에 속하는가?
		- 회귀 : 입력 데이터에서 수치를 예측
			- 사진속 물건의 무게를 예측 => 회귀

### 항등 함수 ( Identify function )
- 입력을 그대로 출력함
	- 입력과 출력이 항상 같음

### 소프트맥스 함수 ( Softmax function )
- 출력층은 모든 입력 신호로 부터 화살표를 받음
	- 출력층의 각 뉴런이 모든 입력신호에서 영향을 받기 때문
```python
def softmax(a):
	exp_a = np.exp(a)
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a

	return y
```

#### 구현시 주의점
- 지수를 이용하므로 오버플로를 유의해야함
	- e^1000은 무한대 inf가 되어 돌아옴
	- 지수함수 계산을 할 때, exp( a + C' )
		- C라는 숫자를 곱해 지수함수로 옮기면 logC가 됨
		- 결국 logC는 임의의 숫자 C'가 됨
```python
def softmax(a):
	c = np.max(a)
	exp_a = np.exp(a - c)
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a

	return y
```

#### 소프트맥스 함수의 특징
- 출력이 0과 1.0 사이
- 함수 출력의 총합은 1 **
	- 이 성질 덕분에 소프트맥스 함수의 출력을 '확률'로 해석 가능함

#### 출력층의 뉴런수
- 풀려는 문제에 맞게 정해야함
	- 분류에서는 분류하고 싶은 클래스의 수로 설정하는 것이 일반적임
		- 클래스 : 분류되는 경우
		- 책 예제의 경우 0-9까지의 10가지 숫자로 분류 될 수 있으므로 10